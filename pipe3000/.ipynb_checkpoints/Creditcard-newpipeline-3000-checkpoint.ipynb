{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "#import missingno as msno\n",
    "\n",
    "from sklearn import decomposition\n",
    "from sklearn import datasets\n",
    "import sklearn as sk\n",
    "\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import cohen_kappa_score\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../creditcard.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a new feature for normal (non-fraudulent) transactions.\n",
    "df.loc[df.Class == 0, 'Normal'] = 1\n",
    "df.loc[df.Class == 1, 'Normal'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Rename 'Class' to 'Fraud'.\n",
    "df = df.rename(columns={'Class': 'Fraud'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#492 fraudulent transactions, 284,315 normal transactions.\n",
    "#0.172% of transactions were fraud. \n",
    "print(df.Normal.value_counts())\n",
    "print()\n",
    "print(df.Fraud.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reduzindo o desbalanceamento por amostragem das trasnsacoes normais frente as fraudulentas\n",
    "Fraud = df[df.Fraud == 1]\n",
    "Normal = df[df.Normal == 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.shape(Fraud))\n",
    "print(np.shape(Normal))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# percentual da amostra para tratar desbalanceamento\n",
    "perc_amostra = 0.01     # 3000 normais"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reduzindo o desbalanceamento por amostragem das trasnsacoes normais frente as fraudulentas\n",
    "# criado 4 grupos de amostragem para verificaçao com diferentes populações"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pupulacao 1\n",
    "Normal_reduzido1 = Normal.sample(frac = perc_amostra)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.shape(Normal_reduzido1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# desconsiderar do universo a populacao 1\n",
    "Normal_reduzido_resto1 = Normal.loc[~Normal.index.isin(Normal_reduzido1.index)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.shape(Normal_reduzido_resto1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pupulacao 2\n",
    "Normal_reduzido2 = Normal_reduzido_resto1.sample(frac = perc_amostra)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.shape(Normal_reduzido2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# desconsiderar do universo reduzido a populacao 2\n",
    "Normal_reduzido_resto2 = Normal_reduzido_resto1.loc[~Normal_reduzido_resto1.index.isin(Normal_reduzido2.index)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.shape(Normal_reduzido_resto2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# populacao 3\n",
    "Normal_reduzido3 = Normal_reduzido_resto2.sample(frac = perc_amostra)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.shape(Normal_reduzido3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# desconsiderar do universo reduzido a populacao 3\n",
    "Normal_reduzido_resto3 = Normal_reduzido_resto2.loc[~Normal_reduzido_resto2.index.isin(Normal_reduzido3.index)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.shape(Normal_reduzido_resto3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# populacao 4\n",
    "Normal_reduzido4 = Normal_reduzido_resto3.sample(frac = perc_amostra)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.shape(Normal_reduzido4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Normal_reduzido1.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualizar pelos valores as amostras no universo dos dados\n",
    "f, ax2 = plt.subplots(1, 1, sharex=True, figsize=(13,6))\n",
    "\n",
    "# universo\n",
    "ax2.scatter(df.Time, df.Amount, s=5, color='steelblue')\n",
    "\n",
    "# populacao 1\n",
    "ax2.scatter(Normal_reduzido1.Time, Normal_reduzido1.Amount, s=10, color='crimson')\n",
    "\n",
    "# populacao 2\n",
    "ax2.scatter(Normal_reduzido2.Time, Normal_reduzido2.Amount, s=10, color='goldenrod')\n",
    "\n",
    "# populacao 3\n",
    "ax2.scatter(Normal_reduzido3.Time, Normal_reduzido3.Amount, s=10, color='darkgreen')\n",
    "\n",
    "# populacao 4\n",
    "ax2.scatter(Normal_reduzido4.Time, Normal_reduzido4.Amount, s=10, color='darkmagenta')\n",
    "\n",
    "plt.xlabel('Time (in Seconds)')\n",
    "plt.ylabel('Amount')\n",
    "plt.ylim((0,8000))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 80% de amostra p treino\n",
    "Normal_reduzido_treino1 = Normal_reduzido1.sample(frac = .8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 20% de amostra p teste\n",
    "Normal_reduzido_test1 = Normal_reduzido1.loc[~Normal_reduzido1.index.isin(Normal_reduzido_treino1.index)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.shape(Normal_reduzido_treino1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.shape(Normal_reduzido_test1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 80% de amostra p treino\n",
    "Normal_reduzido_treino2 = Normal_reduzido2.sample(frac = .8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 20% de amostra p teste\n",
    "Normal_reduzido_test2 = Normal_reduzido2.loc[~Normal_reduzido2.index.isin(Normal_reduzido_treino2.index)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.shape(Normal_reduzido_treino2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.shape(Normal_reduzido_test2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 80% de amostra p treino\n",
    "Normal_reduzido_treino3 = Normal_reduzido3.sample(frac = .8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 20% de amostra p teste\n",
    "Normal_reduzido_test3 = Normal_reduzido3.loc[~Normal_reduzido3.index.isin(Normal_reduzido_treino3.index)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.shape(Normal_reduzido_treino3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.shape(Normal_reduzido_test3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 80% de amostra p treino\n",
    "Normal_reduzido_treino4 = Normal_reduzido4.sample(frac = .8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 20% de amostra p teste\n",
    "Normal_reduzido_test4 = Normal_reduzido4.loc[~Normal_reduzido4.index.isin(Normal_reduzido_treino4.index)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.shape(Normal_reduzido_treino4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.shape(Normal_reduzido_test4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# montagem das amostras das fraudes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 80% de amostra p treino\n",
    "Fraud_treino1 = Fraud.sample(frac = .8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 20% de amostra p teste\n",
    "Fraud_test1 = Fraud.loc[~Fraud.index.isin(Fraud_treino1.index)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.shape(Fraud_treino1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.shape(Fraud_test1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 80% de amostra p treino\n",
    "Fraud_treino2 = Fraud.sample(frac = .8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 20% de amostra p teste\n",
    "Fraud_test2 = Fraud.loc[~Fraud.index.isin(Fraud_treino2.index)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.shape(Fraud_treino2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.shape(Fraud_test2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 80% de amostra p treino\n",
    "Fraud_treino3 = Fraud.sample(frac = .8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 20% de amostra p teste\n",
    "Fraud_test3 = Fraud.loc[~Fraud.index.isin(Fraud_treino3.index)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.shape(Fraud_treino3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.shape(Fraud_test3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 80% de amostra p treino\n",
    "Fraud_treino4 = Fraud.sample(frac = .8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 20% de amostra p teste\n",
    "Fraud_test4 = Fraud.loc[~Fraud.index.isin(Fraud_treino4.index)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.shape(Fraud_treino4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.shape(Fraud_test4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# composicao da amostra com correcao de desbalanceamento\n",
    "X_train1 = pd.concat([Normal_reduzido_treino1, Fraud_treino1], axis = 0)\n",
    "# X_test contains all the transaction not in X_train.\n",
    "X_test1 = pd.concat([Normal_reduzido_test1, Fraud_test1], axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.shape(X_train1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.shape(X_test1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# composicao da amostra com correcao de desbalanceamento\n",
    "X_train2 = pd.concat([Normal_reduzido_treino2, Fraud_treino2], axis = 0)\n",
    "# X_test contains all the transaction not in X_train.\n",
    "X_test2 = pd.concat([Normal_reduzido_test2, Fraud_test2], axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.shape(X_train2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.shape(X_test2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# composicao da amostra com correcao de desbalanceamento\n",
    "X_train3 = pd.concat([Normal_reduzido_treino3, Fraud_treino3], axis = 0)\n",
    "# X_test contains all the transaction not in X_train.\n",
    "X_test3 = pd.concat([Normal_reduzido_test3, Fraud_test3], axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.shape(X_train3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.shape(X_test3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# composicao da amostra com correcao de desbalanceamento\n",
    "X_train4 = pd.concat([Normal_reduzido_treino4, Fraud_treino4], axis = 0)\n",
    "# X_test contains all the transaction not in X_train.\n",
    "X_test4 = pd.concat([Normal_reduzido_test4, Fraud_test4], axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.shape(X_train4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.shape(X_test4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Shuffle the dataframes so that the training is done in a random order.\n",
    "X_train1 = shuffle(X_train1)\n",
    "X_test1 = shuffle(X_test1)\n",
    "\n",
    "X_train2 = shuffle(X_train2)\n",
    "X_test2 = shuffle(X_test2)\n",
    "\n",
    "X_train3 = shuffle(X_train3)\n",
    "X_test3 = shuffle(X_test3)\n",
    "\n",
    "X_train4 = shuffle(X_train4)\n",
    "X_test4 = shuffle(X_test4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.shape(X_train1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Add our target features to y_train and y_test.\n",
    "y_train1 = X_train1.Fraud\n",
    "#y_train1 = pd.concat([y_train1, X_train1.Normal], axis=1)\n",
    "y_test1 = X_test1.Fraud\n",
    "#y_test1 = pd.concat([y_test1, X_test1.Normal], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.shape(y_train1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.shape(y_test1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train2 = X_train2.Fraud\n",
    "#y_train2 = pd.concat([y_train2, X_train2.Normal], axis=1)\n",
    "y_test2 = X_test2.Fraud\n",
    "#y_test2 = pd.concat([y_test2, X_test2.Normal], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.shape(y_train2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.shape(y_test2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train3 = X_train3.Fraud\n",
    "#y_train3 = pd.concat([y_train3, X_train3.Normal], axis=1)\n",
    "y_test3 = X_test3.Fraud\n",
    "#y_test3 = pd.concat([y_test3, X_test3.Normal], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.shape(y_train3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.shape(y_test3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train4 = X_train4.Fraud\n",
    "#y_train4 = pd.concat([y_train4, X_train4.Normal], axis=1)\n",
    "y_test4 = X_test4.Fraud\n",
    "#y_test4 = pd.concat([y_test4, X_test4.Normal], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.shape(y_train4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.shape(y_test4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# montagem dos dados para validacao cruzada\n",
    "X_total1 = pd.concat([X_train1, X_test1], axis = 0)\n",
    "y_total1 = pd.concat([y_train1, y_test1], axis = 0)\n",
    "\n",
    "X_total2 = pd.concat([X_train2, X_test1], axis = 0)\n",
    "y_total2 = pd.concat([y_train2, y_test1], axis = 0)\n",
    "\n",
    "X_total3 = pd.concat([X_train3, X_test1], axis = 0)\n",
    "y_total3 = pd.concat([y_train3, y_test1], axis = 0)\n",
    "\n",
    "X_total4 = pd.concat([X_train4, X_test1], axis = 0)\n",
    "y_total4 = pd.concat([y_train4, y_test1], axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Drop target features from X_total\n",
    "X_total1 = X_total1.drop(['Fraud','Normal'], axis = 1)\n",
    "X_total2 = X_total2.drop(['Fraud','Normal'], axis = 1)\n",
    "X_total3 = X_total3.drop(['Fraud','Normal'], axis = 1)\n",
    "X_total4 = X_total4.drop(['Fraud','Normal'], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.shape(X_total1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_total1.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check to ensure all of the training/testing dataframes are of the correct length\n",
    "print(len(X_total1))\n",
    "print(len(y_total1))\n",
    "\n",
    "print(len(X_total2))\n",
    "print(len(y_total2))\n",
    "\n",
    "print(len(X_total3))\n",
    "print(len(y_total3))\n",
    "\n",
    "print(len(X_total4))\n",
    "print(len(y_total4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#======================================================\n",
    "# montagem dos modelos para o mesmo conjunto amostral\n",
    "#======================================================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Naive Bayes Gaussiano\n",
    "gnb = GaussianNB()\n",
    "\n",
    "# Decision Tree\n",
    "dtc = DecisionTreeClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#======================================================\n",
    "# predicao por validacao cruzada kfold\n",
    "#======================================================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Naive Bayes Gaussiano\n",
    "predicted_gnb1 = cross_val_predict(gnb, X_total1, y_total1, cv=10, n_jobs=-1)\n",
    "predicted_gnb2 = cross_val_predict(gnb, X_total2, y_total2, cv=10, n_jobs=-1)\n",
    "predicted_gnb3 = cross_val_predict(gnb, X_total3, y_total3, cv=10, n_jobs=-1)\n",
    "predicted_gnb4 = cross_val_predict(gnb, X_total4, y_total4, cv=10, n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decision Tree\n",
    "predicted_dtc1 = cross_val_predict(dtc, X_total1, y_total1, cv=10, n_jobs=-1)\n",
    "predicted_dtc2 = cross_val_predict(dtc, X_total2, y_total2, cv=10, n_jobs=-1)\n",
    "predicted_dtc3 = cross_val_predict(dtc, X_total3, y_total3, cv=10, n_jobs=-1)\n",
    "predicted_dtc4 = cross_val_predict(dtc, X_total4, y_total4, cv=10, n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#======================================================\n",
    "# obtencao das metricas da validacao cruzada\n",
    "#======================================================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# acuracia\n",
    "print('---- Cross-Val Accuracy ----')\n",
    "print('Gaussian Naive Bayes')\n",
    "print('Sample 1     %.2f' % metrics.accuracy_score(y_total1, predicted_gnb1))\n",
    "print('Sample 2     %.2f' % metrics.accuracy_score(y_total2, predicted_gnb2))\n",
    "print('Sample 3     %.2f' % metrics.accuracy_score(y_total3, predicted_gnb3))\n",
    "print('Sample 4     %.2f' % metrics.accuracy_score(y_total4, predicted_gnb4))\n",
    "print('')\n",
    "print('Decision Tree Classifier')\n",
    "print('Sample 1     %.2f' % metrics.accuracy_score(y_total1, predicted_dtc1))\n",
    "print('Sample 2     %.2f' % metrics.accuracy_score(y_total2, predicted_dtc2))\n",
    "print('Sample 3     %.2f' % metrics.accuracy_score(y_total3, predicted_dtc3))\n",
    "print('Sample 4     %.2f' % metrics.accuracy_score(y_total4, predicted_dtc4))\n",
    "print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# coeficiente kappa\n",
    "print('---- Kappa coefficient ----')\n",
    "print('Gaussian Naive Bayes')\n",
    "print('Sample 1    %.2f' % cohen_kappa_score(y_total1, predicted_gnb1))\n",
    "print('Sample 2    %.2f' % cohen_kappa_score(y_total2, predicted_gnb2))\n",
    "print('Sample 3    %.2f' % cohen_kappa_score(y_total3, predicted_gnb3))\n",
    "print('Sample 4    %.2f' % cohen_kappa_score(y_total4, predicted_gnb4))\n",
    "print('')\n",
    "print('Decision Tree Classifier')\n",
    "print('Sample 1    %.2f' % cohen_kappa_score(y_total1, predicted_dtc1))\n",
    "print('Sample 2    %.2f' % cohen_kappa_score(y_total2, predicted_dtc2))\n",
    "print('Sample 3    %.2f' % cohen_kappa_score(y_total3, predicted_dtc3))\n",
    "print('Sample 4    %.2f' % cohen_kappa_score(y_total4, predicted_dtc4))\n",
    "print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# matriz de confusao\n",
    "np.set_printoptions(precision=2)\n",
    "\n",
    "# Naive Bayes Gaussiano\n",
    "cnf_matrix_gnb1 = metrics.confusion_matrix(y_total1, predicted_gnb1)\n",
    "cnf_matrix_gnb2 = metrics.confusion_matrix(y_total2, predicted_gnb2)\n",
    "cnf_matrix_gnb3 = metrics.confusion_matrix(y_total3, predicted_gnb3)\n",
    "cnf_matrix_gnb4 = metrics.confusion_matrix(y_total4, predicted_gnb4)\n",
    "\n",
    "# Support Vector Machine\n",
    "cnf_matrix_dtc1 = metrics.confusion_matrix(y_total1, predicted_dtc1)\n",
    "cnf_matrix_dtc2 = metrics.confusion_matrix(y_total2, predicted_dtc2)\n",
    "cnf_matrix_dtc3 = metrics.confusion_matrix(y_total3, predicted_dtc3)\n",
    "cnf_matrix_dtc4 = metrics.confusion_matrix(y_total4, predicted_dtc4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('---- Confusion Matrix ----')\n",
    "print('Gaussian Naive Bayes')\n",
    "print('Sample 1:')\n",
    "print(cnf_matrix_gnb1)\n",
    "print('')\n",
    "print('Sample 2:')\n",
    "print(cnf_matrix_gnb2)\n",
    "print('')\n",
    "print('Sample 3:')\n",
    "print(cnf_matrix_gnb3)\n",
    "print('')\n",
    "print('Sample 4:')\n",
    "print(cnf_matrix_gnb4)\n",
    "print('')\n",
    "\n",
    "print('Support Vector Machine')\n",
    "print('Sample 1:')\n",
    "print(cnf_matrix_dtc1)\n",
    "print('')\n",
    "print('Sample 2:')\n",
    "print(cnf_matrix_dtc2)\n",
    "print('')\n",
    "print('Sample 3:')\n",
    "print(cnf_matrix_dtc3)\n",
    "print('')\n",
    "print('Sample 4:')\n",
    "print(cnf_matrix_dtc4)\n",
    "print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#======================================================\n",
    "# visualizacao das matrizes de confusao\n",
    "#======================================================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn import svm, datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "    \n",
    "    \n",
    "    \n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=0)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "    print(cm)\n",
    "\n",
    "    thresh = cm.max() / 2.\n",
    "    np.set_printoptions(precision=2)\n",
    "    print(thresh)\n",
    "    \n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        \n",
    "        if normalize:\n",
    "            plt.text(j, i, '%.3f' % cm[i, j],\n",
    "                 horizontalalignment=\"center\", fontsize=15,\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "        else:\n",
    "            plt.text(j, i, '%.0f' % cm[i, j],\n",
    "                 horizontalalignment=\"center\", fontsize=15,\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "        \n",
    "        \n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names = ['Normal', 'Fraud']\n",
    "#class_names = ['Fraud','Normal']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Naive Bayes Gaussiano\n",
    "\n",
    "# Plot normalized confusion matrix\n",
    "#plt.figure()\n",
    "#plot_confusion_matrix(cnf_matrix_gnb, classes=class_names, normalize=True,\n",
    "#                      title='Normalized confusion matrix')\n",
    "#plt.savefig('GNB-Normalized.png')\n",
    "\n",
    "# Plot non-normalized confusion matrix\n",
    "plt.figure(figsize=(11,6.))\n",
    "plt.subplot(221)\n",
    "plot_confusion_matrix(cnf_matrix_gnb1, classes=class_names,\n",
    "                      title='Confusion matrix, sample 1')\n",
    "\n",
    "#plt.savefig('GNB1-Non-Normalized.png')\n",
    "\n",
    "# Plot non-normalized confusion matrix\n",
    "#plt.figure()\n",
    "plt.subplot(222)\n",
    "plot_confusion_matrix(cnf_matrix_gnb2, classes=class_names,\n",
    "                      title='Confusion matrix, sample 2')\n",
    "#plt.savefig('GNB2-Non-Normalized.png')\n",
    "\n",
    "# Plot non-normalized confusion matrix\n",
    "#plt.figure()\n",
    "plt.subplot(223)\n",
    "plot_confusion_matrix(cnf_matrix_gnb3, classes=class_names,\n",
    "                      title='Confusion matrix, sample 3')\n",
    "#plt.savefig('GNB3-Non-Normalized.png')\n",
    "\n",
    "# Plot non-normalized confusion matrix\n",
    "#plt.figure()\n",
    "plt.subplot(224)\n",
    "plot_confusion_matrix(cnf_matrix_gnb4, classes=class_names,\n",
    "                      title='Confusion matrix, sample 4')\n",
    "plt.savefig('GNB-Non-Normalized.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decision Tree Classifier\n",
    "\n",
    "# Plot normalized confusion matrix\n",
    "#plt.figure()\n",
    "#plot_confusion_matrix(cnf_matrix_dtc, classes=class_names, normalize=True,\n",
    "#                      title='Normalized confusion matrix')\n",
    "#plt.savefig('GNB-Normalized.png')\n",
    "\n",
    "# Plot non-normalized confusion matrix\n",
    "plt.figure(figsize=(11,6.))\n",
    "plt.subplot(221)\n",
    "plot_confusion_matrix(cnf_matrix_dtc1, classes=class_names,\n",
    "                      title='Confusion matrix, sample 1')\n",
    "#plt.savefig('DTC1-Non-Normalized.png')\n",
    "\n",
    "# Plot non-normalized confusion matrix\n",
    "plt.subplot(222)\n",
    "plot_confusion_matrix(cnf_matrix_dtc2, classes=class_names,\n",
    "                      title='Confusion matrix, sample 2')\n",
    "#plt.savefig('DTC2-Non-Normalized.png')\n",
    "\n",
    "# Plot non-normalized confusion matrix\n",
    "plt.subplot(223)\n",
    "plot_confusion_matrix(cnf_matrix_dtc3, classes=class_names,\n",
    "                      title='Confusion matrix, sample 3')\n",
    "#plt.savefig('DTC3-Non-Normalized.png')\n",
    "\n",
    "# Plot non-normalized confusion matrix\n",
    "plt.subplot(224)\n",
    "plot_confusion_matrix(cnf_matrix_dtc4, classes=class_names,\n",
    "                      title='Confusion matrix, sample 4')\n",
    "plt.savefig('DTC-Non-Normalized.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#======================================================\n",
    "# montagem e plot do espaco roc\n",
    "#======================================================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Naive Bayes Gaussiano\n",
    "fpr_gnb1, tpr_gnb1, thresholds_gnb1 = metrics.roc_curve(y_total1, predicted_gnb1)\n",
    "roc_auc_gnb1 = auc(fpr_gnb1, tpr_gnb1)\n",
    "\n",
    "plt.plot(fpr_gnb1, tpr_gnb1, lw=2,label='ROC-GNB1 (area = %0.2f)' % ( roc_auc_gnb1))\n",
    "\n",
    "plt.xlim([-0.05, 1.05])\n",
    "plt.ylim([-0.05, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver operating characteristic example')\n",
    "plt.legend(loc=\"lower right\")\n",
    "#plt.show()\n",
    "\n",
    "#plt.savefig('ROC-GNB1.png')\n",
    "\n",
    "\n",
    "# Naive Bayes Gaussiano\n",
    "fpr_gnb2, tpr_gnb2, thresholds_gnb2 = metrics.roc_curve(y_total2, predicted_gnb2)\n",
    "roc_auc_gnb2 = auc(fpr_gnb2, tpr_gnb2)\n",
    "\n",
    "plt.plot(fpr_gnb2, tpr_gnb2, lw=2,label='ROC-GNB2 (area = %0.2f)' % ( roc_auc_gnb2))\n",
    "\n",
    "plt.xlim([-0.05, 1.05])\n",
    "plt.ylim([-0.05, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver operating characteristic example')\n",
    "plt.legend(loc=\"lower right\")\n",
    "#plt.show()\n",
    "\n",
    "#plt.savefig('ROC-GNB2.png')\n",
    "\n",
    "\n",
    "# Naive Bayes Gaussiano\n",
    "fpr_gnb3, tpr_gnb3, thresholds_gnb3 = metrics.roc_curve(y_total3, predicted_gnb3)\n",
    "roc_auc_gnb3 = auc(fpr_gnb3, tpr_gnb3)\n",
    "\n",
    "plt.plot(fpr_gnb3, tpr_gnb3, lw=2,label='ROC-GNB3 (area = %0.2f)' % ( roc_auc_gnb3))\n",
    "\n",
    "plt.xlim([-0.05, 1.05])\n",
    "plt.ylim([-0.05, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver operating characteristic example')\n",
    "plt.legend(loc=\"lower right\")\n",
    "#plt.show()\n",
    "\n",
    "#plt.savefig('ROC-GNB3.png')\n",
    "\n",
    "\n",
    "# Naive Bayes Gaussiano\n",
    "fpr_gnb4, tpr_gnb4, thresholds_gnb4 = metrics.roc_curve(y_total4, predicted_gnb4)\n",
    "roc_auc_gnb4 = auc(fpr_gnb4, tpr_gnb4)\n",
    "\n",
    "plt.plot(fpr_gnb4, tpr_gnb4, lw=2,label='ROC-GNB4 (area = %0.2f)' % ( roc_auc_gnb4))\n",
    "plt.plot([0, 1], [0, 1], linestyle='--', lw=2, color='k',\n",
    "         label='Luck')\n",
    "plt.xlim([-0.05, 1.05])\n",
    "plt.ylim([-0.05, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver operating characteristic example')\n",
    "plt.legend(loc=\"lower right\")\n",
    "#plt.show()\n",
    "\n",
    "plt.savefig('ROC-GNB.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decision Tree Classifier\n",
    "fpr_dtc1, tpr_dtc1, thresholds_dtc1 = metrics.roc_curve(y_total1, predicted_dtc1)\n",
    "roc_auc_dtc1 = auc(fpr_dtc1, tpr_dtc1)\n",
    "\n",
    "plt.plot(fpr_dtc1, tpr_dtc1, lw=2,label='ROC-DTC1 (area = %0.2f)' % ( roc_auc_dtc1))\n",
    "\n",
    "plt.xlim([-0.05, 1.05])\n",
    "plt.ylim([-0.05, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver operating characteristic example')\n",
    "plt.legend(loc=\"lower right\")\n",
    "#plt.show()\n",
    "\n",
    "#plt.savefig('ROC-DTC1.png')\n",
    "\n",
    "\n",
    "# Decision Tree Classifier\n",
    "fpr_dtc2, tpr_dtc2, thresholds_dtc2 = metrics.roc_curve(y_total2, predicted_dtc2)\n",
    "roc_auc_dtc2 = auc(fpr_dtc2, tpr_dtc2)\n",
    "\n",
    "plt.plot(fpr_dtc2, tpr_dtc2, lw=2,label='ROC-DTC2 (area = %0.2f)' % ( roc_auc_dtc2))\n",
    "\n",
    "plt.xlim([-0.05, 1.05])\n",
    "plt.ylim([-0.05, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver operating characteristic example')\n",
    "plt.legend(loc=\"lower right\")\n",
    "#plt.show()\n",
    "\n",
    "#plt.savefig('ROC-DTC2.png')\n",
    "\n",
    "\n",
    "# Decision Tree Classifier\n",
    "fpr_dtc3, tpr_dtc3, thresholds_dtc3 = metrics.roc_curve(y_total3, predicted_dtc3)\n",
    "roc_auc_dtc3 = auc(fpr_dtc3, tpr_dtc3)\n",
    "\n",
    "plt.plot(fpr_dtc3, tpr_dtc3, lw=2,label='ROC-DTC3 (area = %0.2f)' % ( roc_auc_dtc3))\n",
    "\n",
    "plt.xlim([-0.05, 1.05])\n",
    "plt.ylim([-0.05, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver operating characteristic example')\n",
    "plt.legend(loc=\"lower right\")\n",
    "#plt.show()\n",
    "\n",
    "#plt.savefig('ROC-DTC3.png')\n",
    "\n",
    "\n",
    "# Decision Tree Classifier\n",
    "fpr_dtc4, tpr_dtc4, thresholds_dtc4 = metrics.roc_curve(y_total4, predicted_dtc4)\n",
    "roc_auc_dtc4 = auc(fpr_dtc4, tpr_dtc4)\n",
    "\n",
    "plt.plot(fpr_dtc4, tpr_dtc4, lw=2,label='ROC-DTC4 (area = %0.2f)' % ( roc_auc_dtc4))\n",
    "plt.plot([0, 1], [0, 1], linestyle='--', lw=2, color='k',\n",
    "         label='Luck')\n",
    "plt.xlim([-0.05, 1.05])\n",
    "plt.ylim([-0.05, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver operating characteristic example')\n",
    "plt.legend(loc=\"lower right\")\n",
    "#plt.show()\n",
    "\n",
    "plt.savefig('ROC-DTC.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
